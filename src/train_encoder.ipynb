{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Code to convoxel this notebook to .py if you want to run it via command line or with Slurm\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvoxel train_encoder.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 2225470\n",
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom models and functions #\n",
    "import utils\n",
    "from models import Clipper, BrainNetwork, BrainDiffusionPrior, VersatileDiffusionPriorNetwork\n",
    "\n",
    "# Multi-GPU config #\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator(split_batches=False,mixed_precision='fp16')  \n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--clip_variant=ViT-L/14', '--batch_size=32', '--n_samples_save=0', '--max_lr=3e-4', '--mixup_pct=.33', '--num_epochs=240', '--ckpt_interval=5', '--no-use_image_aug', '--prior_mult=30']\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    jupyter_args = \"--clip_variant=ViT-L/14 --batch_size=32 --n_samples_save=0 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=240 --ckpt_interval=5 --no-use_image_aug --prior_mult=30\"\n",
    "    \n",
    "    jupyter_args = jupyter_args.split()\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    %autoreload 2 # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--modality\", type=str, default=\"image\", choices=[\"image\", \"text\"],\n",
    "    help=\"image or text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-medarc/fmri/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored (see README)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,5,7],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--voxel2clip_path\", type=str, default=\"None\",\n",
    "    help=\"pretrained checkpoint to initialize voxel2clip\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--versatile\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"if True, CLIP embeddings will come from last hidden layer (e.g., 257x768 - Versatile Diffusion), rather than final layer\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from InfoNCE to soft_clip_loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--norm_embs\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Do norming (using cls token if VD) of CLIP embeddings\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation (only used for modality=image)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=300,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"if False, only train via NCE loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--v2c\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"if False, only train via diffusion prior loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--plot_umap\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Plot UMAP plots alongside reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if True, saves best.ckpt at end of training. if False and ckpt_saving==True, will save best.ckpt whenever epoch shows best validation score\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_mult\",type=float,default=30,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_samples_save\",type=int,default=0,\n",
    "    help=\"Number of reconstructions for monitoring progress, 0 will speed up training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_projector\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Additional MLP after the main MLP so model can separately learn a way to minimize NCE from prior loss (BYOL)\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "# change learning rate based on number of devices\n",
    "max_lr *= accelerator.num_processes\n",
    "    \n",
    "# change batch size based on number of devices if using multi-gpu\n",
    "# batch_size *= accelerator.num_processes\n",
    "\n",
    "# change num_epochs based on number of devices if using multi-gpu\n",
    "num_epochs *= accelerator.num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "if use_image_aug:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "if modality=='text':\n",
    "    annots = np.load(f\"{data_path}/COCO_73k_annots_curated.npy\")\n",
    "    import logging\n",
    "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "    from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    text_encoder = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "validation: num_worker_batches 10\n"
     ]
    }
   ],
   "source": [
    "print('Pulling NSD webdataset data...')\n",
    "# Note: using \"voxel\" naming even though we use vertices here... makes it easier porting over MindEye lingo\n",
    "\n",
    "train_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/subj01_{2..98}.tar\"\n",
    "val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/subj01_{0..2}.tar\"\n",
    "meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/metadata_subj01.json\"\n",
    "\n",
    "metadata = json.load(open(meta_url))\n",
    "num_train = metadata['total'] - 300\n",
    "num_val = 300\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "import math\n",
    "import random\n",
    "import webdataset as wds\n",
    "def my_split_by_node(urls):\n",
    "    return urls\n",
    "\n",
    "global_batch_size = batch_size * num_devices\n",
    "num_batches = math.floor(num_train / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "\n",
    "train_data = wds.WebDataset(train_url, resampled=False)\\\n",
    "    .shuffle(500, initial=500, rng=random.Random(seed))\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=\"vert.npy\", latent=\"clip_emb_hidden.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"latent\")\\\n",
    "    .batched(batch_size, partial=False)\\\n",
    "    .with_epoch(num_worker_batches)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_data, num_workers=num_workers,\n",
    "                        batch_size=None, shuffle=False, persistent_workers=True)\n",
    "\n",
    "global_batch_size = batch_size\n",
    "num_workers = 1\n",
    "\n",
    "num_batches = math.ceil(num_val / global_batch_size)\n",
    "num_worker_batches = math.ceil(num_batches / num_workers)\n",
    "print(\"validation: num_worker_batches\", num_worker_batches)\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False, nodesplitter=my_split_by_node)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=\"vert.npy\", latent=\"clip_emb_hidden.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"latent\")\\\n",
    "    .batched(batch_size, partial=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_data, num_workers=num_workers,\n",
    "                    batch_size=None, shuffle=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating clip2voxel...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clip_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m subj \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m     17\u001b[0m     num_voxels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m99\u001b[39m\n\u001b[0;32m---> 18\u001b[0m voxel2clip_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(in_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m257\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m768\u001b[39m,out_dim\u001b[38;5;241m=\u001b[39mnum_voxels,clip_size\u001b[38;5;241m=\u001b[39m\u001b[43mclip_size\u001b[49m,use_projector\u001b[38;5;241m=\u001b[39muse_projector)\n\u001b[1;32m     19\u001b[0m voxel2clip \u001b[38;5;241m=\u001b[39m BrainNetwork(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvoxel2clip_kwargs)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# load from ckpt\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_size' is not defined"
     ]
    }
   ],
   "source": [
    "print('Creating clip2voxel...')\n",
    "if subj == 1:\n",
    "    num_voxels = 39548\n",
    "elif subj == 2:\n",
    "    num_voxels = 99\n",
    "elif subj == 3:\n",
    "    num_voxels = 99\n",
    "elif subj == 4:\n",
    "    num_voxels = 99\n",
    "elif subj == 5:\n",
    "    num_voxels = 99\n",
    "elif subj == 6:\n",
    "    num_voxels = 99\n",
    "elif subj == 7:\n",
    "    num_voxels = 99\n",
    "elif subj == 8:\n",
    "    num_voxels = 99\n",
    "voxel2clip_kwargs = dict(in_dim=257*768,out_dim=num_voxels,clip_size=clip_size,use_projector=use_projector)\n",
    "voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "    \n",
    "# load from ckpt\n",
    "if voxel2clip_path!=\"None\":\n",
    "    checkpoint = torch.load(voxel2clip_path, map_location='cpu')\n",
    "    voxel2clip.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    del checkpoint\n",
    "    \n",
    "print(\"params of voxel2clip:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(voxel2clip)\n",
    "    \n",
    "# setup prior network\n",
    "out_dim = clip_size\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = clip_size//64 # heads * dim_head = 12 * 64 = 768\n",
    "guidance_scale = 3.5\n",
    "timesteps = 100\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = 257,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    ).to(device)\n",
    "print(\"prior_network loaded\")\n",
    "\n",
    "# custom version that can fix seeds\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=voxel2clip,\n",
    ").to(device)\n",
    "\n",
    "if not prior:\n",
    "    diffusion_prior = diffusion_prior.requires_grad_(False)\n",
    "    diffusion_prior.voxel2clip.requires_grad_(True)\n",
    "\n",
    "print(\"params of diffusion prior:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(diffusion_prior)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in diffusion_prior.net.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in diffusion_prior.net.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "global_batch_size = batch_size * num_devices\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(num_epochs*(num_train//global_batch_size)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(num_epochs*(num_train//global_batch_size))\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):    \n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    unwrapped_model = accelerator.unwrap_model(diffusion_prior)\n",
    "    try:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'val_losses': val_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    except:\n",
    "        print(\"Couldn't save... moving on to prevent crashing.\")\n",
    "    del unwrapped_model\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for wandb\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    \n",
    "    wandb_project = 'stability'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "    wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"modality\": modality,\n",
    "      \"clip_variant\": clip_variant,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"lr_scheduler_type\": lr_scheduler_type,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_train\": num_train,\n",
    "      \"num_val\": num_val,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"val_url\": val_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    if True: # wandb_auto_resume\n",
    "        print(\"wandb_id:\",model_name)\n",
    "        wandb.init(\n",
    "            id = model_name,\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            resume=\"allow\",\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "nce_losses, val_nce_losses = [], []\n",
    "sim_losses, val_sim_losses = [], []\n",
    "best_val_loss = 1e9\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "val_voxel0 = val_image0 = None\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    try:\n",
    "        checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    except:\n",
    "        print('last.pth failed... trying last_backup.pth')\n",
    "        checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(\"Epoch\",epoch)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    diffusion_prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "    del checkpoint\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "        try:\n",
    "            checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "        except:\n",
    "            print('last.pth failed... trying last_backup.pth')\n",
    "            checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        diffusion_prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "        del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ae0ca-02c2-43d1-b20a-d1a33801873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_prior, optimizer, train_dl, val_dl, lr_scheduler = accelerator.prepare(\n",
    "diffusion_prior, optimizer, train_dl, val_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077f96f-fd56-48b4-b5fd-842fbc2af2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (voxel,image,clip) in enumerate(val_dl):\n",
    "    print(image.shape,image.device)\n",
    "    print(voxel.shape)\n",
    "    print(clip.shape)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(utils.torch_to_Image(image))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a3368c-e6ce-49cc-b970-ee3dba12dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 10/240 [40:37<13:46:07, 215.51s/it, train/bwd_pct_correct=tensor(0.7254, device='cuda:0'), train/cosine_sim_base=0.0263, train/fwd_pct_correct=tensor(0.6743, device='cuda:0'), train/loss=1.07, train/loss_nce=0.508, train/loss_prior=0.186, train/lr=0.000299, train/num_steps=3245, val/cosine_sim_base=0.043, val/loss=0.66, val/loss_nce=0.147, val/loss_prior=0.171, val/num_steps=99, val/val_bwd_pct_correct=tensor(0.9549, device='cuda:0'), val/val_fwd_pct_correct=tensor(0.9792, device='cuda:0')]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /fsx/proj-medarc/fmri/paulscotti/fMRI-Algonauts-Challenge-2023/train_logs/testing/last.pth\n",
      "saving /fsx/proj-medarc/fmri/paulscotti/fMRI-Algonauts-Challenge-2023/train_logs/testing/last_backup.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████████████████████████████▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 11/240 [41:26<14:06:03, 221.68s/it, train/bwd_pct_correct=tensor(0.7254, device='cuda:0'), train/cosine_sim_base=0.0263, train/fwd_pct_correct=tensor(0.6743, device='cuda:0'), train/loss=1.07, train/loss_nce=0.508, train/loss_prior=0.186, train/lr=0.000299, train/num_steps=3245, val/cosine_sim_base=0.043, val/loss=0.66, val/loss_nce=0.147, val/loss_prior=0.171, val/num_steps=99, val/val_bwd_pct_correct=tensor(0.9549, device='cuda:0'), val/val_fwd_pct_correct=tensor(0.9792, device='cuda:0')]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't save... moving on to prevent crashing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████████████████████████████▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 11/240 [41:31<14:24:37, 226.54s/it, train/bwd_pct_correct=tensor(0.7254, device='cuda:0'), train/cosine_sim_base=0.0263, train/fwd_pct_correct=tensor(0.6743, device='cuda:0'), train/loss=1.07, train/loss_nce=0.508, train/loss_prior=0.186, train/lr=0.000299, train/num_steps=3245, val/cosine_sim_base=0.043, val/loss=0.66, val/loss_nce=0.147, val/loss_prior=0.171, val/num_steps=99, val/val_bwd_pct_correct=tensor(0.9549, device='cuda:0'), val/val_fwd_pct_correct=tensor(0.9792, device='cuda:0')]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2170787) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/multiprocessing/queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m val_loss_nce_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     16\u001b[0m val_loss_prior_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_i, (voxel, image, latent) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dl):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m     20\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/accelerate/data_loader.py:509\u001b[0m, in \u001b[0;36mDataLoaderDispatcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m main_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mprocess_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# We only iterate through the DataLoader on process 0.\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     main_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m stop_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1111\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1109\u001b[0m resume_iteration_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1111\u001b[0m     return_idx, return_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_ResumeIteration):\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2170787) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    diffusion_prior.train()\n",
    "\n",
    "    sims_base = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "    loss_nce_sum = 0.\n",
    "    loss_prior_sum = 0.\n",
    "    val_loss_nce_sum = 0.\n",
    "    val_loss_prior_sum = 0.\n",
    "\n",
    "    for train_i, (voxel, image, latent) in enumerate(train_dl):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            optimizer.zero_grad()\n",
    "            repeat_index = train_i % 3\n",
    "            voxel = voxel[:,repeat_index].float()\n",
    "            \n",
    "            if use_image_aug:\n",
    "                image = img_augment(image)\n",
    "                # plt.imshow(utils.torch_to_Image(image))\n",
    "                # plt.show()\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "            # clip_target = clip_extractor.embed_image(image).float()\n",
    "            clip_target = latent.to(device).float().squeeze(1)\n",
    "\n",
    "            clip_voxels, clip_voxels_proj = diffusion_prior.module.voxel2clip(voxel) if distributed else diffusion_prior.voxel2clip(voxel)\n",
    "            if versatile:\n",
    "                clip_voxels = clip_voxels.view(len(voxel),-1,clip_size)\n",
    "            \n",
    "            if prior:\n",
    "                loss_prior, aligned_clip_voxels = diffusion_prior(text_embed=clip_voxels, image_embed=clip_target)\n",
    "                aligned_clip_voxels /= diffusion_prior.module.image_embed_scale if distributed else diffusion_prior.image_embed_scale\n",
    "            else:\n",
    "                aligned_clip_voxels = clip_voxels\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels_proj.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                loss_nce = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006, \n",
    "                    perm=perm, betas=betas, select=select)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                loss_nce = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=epoch_temp)\n",
    "                \n",
    "            if prior and v2c:\n",
    "                loss_nce_sum += loss_nce.item()\n",
    "                loss_prior_sum += loss_prior.item()\n",
    "                loss = loss_nce + (prior_mult * loss_prior)\n",
    "            elif v2c:\n",
    "                loss_nce_sum += loss_nce.item()\n",
    "                loss = loss_nce\n",
    "            elif prior:\n",
    "                loss_prior_sum += loss_prior.item()\n",
    "                loss = prior_mult * loss_prior\n",
    "            utils.check_loss(loss)\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # gather batches across multi-gpu if there's multiple\n",
    "            # clip_voxel_gather = accelerator.gather(clip_voxels_norm.view(len(voxel),-1).contiguous())\n",
    "            # clip_target_gather = accelerator.gather(clip_target_norm.view(len(voxel),-1).contiguous())\n",
    "\n",
    "            sims_base += nn.functional.cosine_similarity(clip_target_norm,clip_voxels_norm).mean().item()\n",
    "\n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_target_norm)).to(device) \n",
    "            fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1)\n",
    "            bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1)\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    diffusion_prior.eval()\n",
    "    for val_i, (voxel, image, latent) in enumerate(val_dl): \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                voxel = torch.mean(voxel,axis=1)\n",
    "                \n",
    "                if use_image_aug:\n",
    "                    image = img_augment(image)\n",
    "\n",
    "                if val_image0 is None:\n",
    "                    val_image0 = image.detach().clone()\n",
    "                    val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "                clip_target = latent.to(device).float().squeeze(1)\n",
    "\n",
    "                clip_voxels, clip_voxels_proj = diffusion_prior.module.voxel2clip(voxel) if distributed else diffusion_prior.voxel2clip(voxel)\n",
    "                if versatile:\n",
    "                    clip_voxels = clip_voxels.view(len(voxel),-1,clip_size)\n",
    "                \n",
    "                if prior:\n",
    "                    val_loss_prior, aligned_clip_voxels = diffusion_prior(text_embed=clip_voxels, image_embed=clip_target)\n",
    "                    aligned_clip_voxels /= diffusion_prior.module.image_embed_scale if distributed else diffusion_prior.image_embed_scale\n",
    "                else:\n",
    "                    aligned_clip_voxels = clip_voxels\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels_proj.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    val_loss_nce = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006, \n",
    "                        perm=None, betas=None, select=None)\n",
    "                else:\n",
    "                    val_loss_nce = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                if prior and v2c:\n",
    "                    val_loss_nce_sum += val_loss_nce.item()\n",
    "                    val_loss_prior_sum += val_loss_prior.item()\n",
    "                    val_loss = val_loss_nce + (prior_mult * val_loss_prior)\n",
    "                elif v2c:\n",
    "                    val_loss_nce_sum += val_loss_nce.item()\n",
    "                    val_loss = val_loss_nce\n",
    "                elif prior:\n",
    "                    val_loss_prior_sum += val_loss_prior.item()\n",
    "                    val_loss = prior_mult * val_loss_prior\n",
    "                utils.check_loss(val_loss)\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                # clip_voxel_gather = accelerator.gather(clip_voxels_norm.view(len(voxel),-1).contiguous())\n",
    "                # clip_target_gather = accelerator.gather(clip_target_norm.view(len(voxel),-1).contiguous())\n",
    "\n",
    "                val_sims_base += nn.functional.cosine_similarity(clip_target_norm,clip_voxels_norm).mean().item()\n",
    "                \n",
    "                labels = torch.arange(len(clip_target_norm)).to(device)\n",
    "                val_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1)\n",
    "                val_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1)\n",
    "\n",
    "    if local_rank==0:        \n",
    "        if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "                \n",
    "        if utils.is_interactive():\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"val/num_steps\": len(val_losses),\n",
    "            \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "            \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "            \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "            \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "            \"train/loss_nce\": loss_nce_sum / (train_i + 1),\n",
    "            \"train/loss_prior\": loss_prior_sum / (train_i + 1),\n",
    "            \"val/loss_nce\": val_loss_nce_sum / (val_i + 1),\n",
    "            \"val/loss_prior\": val_loss_prior_sum / (val_i + 1)}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        # Save model checkpoint and reconstruct\n",
    "        save_ckpt(f'last')\n",
    "        if epoch % ckpt_interval == 0:\n",
    "            save_ckpt(f'last_backup')\n",
    "                \n",
    "        if wandb_log: wandb.log(logs)\n",
    "        \n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70795f27-fb5f-4281-b437-248b5c525183",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = clip_extractor.embed_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677cb0b-8d46-4e32-a953-84dbd1a095cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21491bfc-0135-456c-8625-46bdd19c5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cea66b-42d6-4227-885b-9c9352658f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 4 epochs\n",
    "# train/loss=14, train/loss_nce=1.69, train/loss_prior=41\n",
    "# val/loss=14.8, val/loss_nce=3.89, val/loss_prior=36.3\n",
    "# val/loss=12.9, val/loss_nce=3.94, val/loss_prior=30\n",
    "# val/loss=11.2, val/loss_nce=3.99, val/loss_prior=24.1\n",
    "# val/loss=11, val/loss_nce=3.94, val/loss_prior=23.4\n",
    "# val/loss=10.1, val/loss_nce=3.87, val/loss_prior=20.8\n",
    "# val/loss=10.1, val/loss_nce=3.94, val/loss_prior=20.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
