{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "- Improve training features (accelerator, lr scheduler, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "validation: num_worker_batches 1\n"
     ]
    }
   ],
   "source": [
    "print('Pulling NSD webdataset data...')\n",
    "# Note: using \"voxel\" naming even though we use vertices here... makes it easier porting over MindEye lingo\n",
    "\n",
    "train_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/subj01_{3..98}.tar\"\n",
    "val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/subj01_{0..2}.tar\"\n",
    "meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/metadata_subj01.json\"\n",
    "\n",
    "metadata = json.load(open(meta_url))\n",
    "num_train = metadata['total'] - 300\n",
    "num_val = 300\n",
    "batch_size = 32\n",
    "num_devices = 1\n",
    "seed = 42\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "import math\n",
    "import random\n",
    "import webdataset as wds\n",
    "def my_split_by_node(urls):\n",
    "    return urls\n",
    "\n",
    "num_workers = 10\n",
    "\n",
    "global_batch_size = batch_size * num_devices\n",
    "num_batches = math.floor(num_train / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "\n",
    "train_data = wds.WebDataset(train_url, resampled=False)\\\n",
    "    .shuffle(500, initial=500, rng=random.Random(seed))\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=\"vert.npy\", clip_latent=\"clip_emb_final.npy\", clip_last_hidden=\"clip_emb_hidden.npy\", imagebind_latent='imagebind_final.npy', imagebinde_hidden='imagebind_hidden.npy')\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"clip_latent\", \"clip_last_hidden\", \"imagebind_latent\", \"imagebinde_hidden\")\\\n",
    "    .batched(batch_size, partial=False)\\\n",
    "    .with_epoch(num_worker_batches)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_data, num_workers=num_workers,\n",
    "                        batch_size=None, shuffle=False, persistent_workers=True)\n",
    "\n",
    "global_batch_size = batch_size\n",
    "num_workers_val = 1\n",
    "\n",
    "num_batches_val = math.ceil(num_val / global_batch_size)\n",
    "num_worker_batches_val = math.ceil(num_batches_val / num_workers)\n",
    "print(\"validation: num_worker_batches\", num_worker_batches_val)\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False, nodesplitter=my_split_by_node)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=\"vert.npy\", clip_latent=\"clip_emb_final.npy\", clip_last_hidden=\"clip_emb_hidden.npy\", imagebind_latent='imagebind_final.npy', imagebinde_hidden='imagebind_hidden.npy')\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"clip_latent\", \"clip_last_hidden\", \"imagebind_latent\", \"imagebinde_hidden\")\\\n",
    "    .batched(300, partial=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_data, num_workers=num_workers_val,\n",
    "                    batch_size=None, shuffle=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader shapes:\n",
      "voxels torch.Size([32, 3, 39548])\n",
      "images torch.Size([32, 3, 425, 425])\n",
      "clip_latent torch.Size([32, 1, 768])\n",
      "clip_last_hidden torch.Size([32, 1, 257, 768])\n",
      "imagebind_latent torch.Size([32, 1, 1024])\n",
      "imagebind_hidden torch.Size([32, 1, 257, 1280])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "voxels_shape, images_shape, clip_latent_shape, clip_last_hidden_shape, imagebind_latent_shape, imagebind_hidden_shape = None, None, None, None, None, None\n",
    "for voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden in tqdm(train_dl):\n",
    "    voxels_shape = voxels.shape\n",
    "    images_shape = images.shape\n",
    "    clip_latent_shape = clip_latent.shape\n",
    "    clip_last_hidden_shape = clip_last_hidden.shape\n",
    "    imagebind_latent_shape = imagebind_latent.shape\n",
    "    imagebind_hidden_shape = imagebind_hidden.shape\n",
    "    break\n",
    "\n",
    "print('Train dataloader shapes:')\n",
    "print('voxels', voxels_shape)\n",
    "print('images', images_shape)\n",
    "print('clip_latent', clip_latent_shape)\n",
    "print('clip_last_hidden', clip_last_hidden_shape)\n",
    "print('imagebind_latent', imagebind_latent_shape)\n",
    "print('imagebind_hidden', imagebind_hidden_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader shapes:\n",
      "voxels torch.Size([300, 3, 39548])\n",
      "images torch.Size([300, 3, 425, 425])\n",
      "clip_latent torch.Size([300, 1, 768])\n",
      "clip_last_hidden torch.Size([300, 1, 257, 768])\n",
      "imagebind_latent torch.Size([300, 1, 1024])\n",
      "imagebind_hidden torch.Size([300, 1, 257, 1280])\n"
     ]
    }
   ],
   "source": [
    "voxels_shape, images_shape, clip_latent_shape, clip_last_hidden_shape, imagebind_latent_shape, imagebind_hidden_shape = None, None, None, None, None, None\n",
    "for voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden in val_dl:\n",
    "    voxels_shape = voxels.shape\n",
    "    images_shape = images.shape\n",
    "    clip_latent_shape = clip_latent.shape\n",
    "    clip_last_hidden_shape = clip_last_hidden.shape\n",
    "    imagebind_latent_shape = imagebind_latent.shape\n",
    "    imagebind_hidden_shape = imagebind_hidden.shape\n",
    "    break\n",
    "\n",
    "print('Train dataloader shapes:')\n",
    "print('voxels', voxels_shape)\n",
    "print('images', images_shape)\n",
    "print('clip_latent', clip_latent_shape)\n",
    "print('clip_last_hidden', clip_last_hidden_shape)\n",
    "print('imagebind_latent', imagebind_latent_shape)\n",
    "print('imagebind_hidden', imagebind_hidden_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "alpha = 1e-8\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, alpha=1.0, hidden_size=3000):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_in = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear_hid = torch.nn.Linear(hidden_size, output_size)\n",
    "        # self.linear_out = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_hid(self.linear_in(x))\n",
    "\n",
    "    def l2_regularization(self):\n",
    "        l2_reg = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.sum(torch.pow(param, 2))\n",
    "        return self.alpha * l2_reg\n",
    "\n",
    "\n",
    "def train_the_model_and_get_predictions(map_name, model, save = True):\n",
    "    # outputs name can be 'clip_latent', 'clip_last_hidden', 'imagebind_latent', 'imagebind_hidden'\n",
    "    map_shape = None\n",
    "    if map_name == 'clip_latent':\n",
    "        map_shape = clip_latent_shape\n",
    "    elif map_name == 'clip_last_hidden':\n",
    "        map_shape = (clip_last_hidden_shape[0], clip_last_hidden_shape[1], clip_last_hidden_shape[2] * clip_last_hidden_shape[3])\n",
    "    elif map_name == 'imagebind_latent':\n",
    "        map_shape = imagebind_latent_shape\n",
    "    elif map_name == 'imagebind_last_hidden':\n",
    "        map_shape = (clip_last_hidden_shape[0], imagebind_hidden_shape[1], imagebind_hidden_shape[2] * imagebind_hidden_shape[3])\n",
    "    else:\n",
    "        raise Exception('Invalid map_name')\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    best_loss = 1000000\n",
    "\n",
    "    # Train the right hemisphere model\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden) in enumerate(train_dl):\n",
    "\n",
    "            # Move tensors to the configured device\n",
    "            labels = voxels.to(device).float().mean(dim=1)  # TODO: I'm just taking the mean, Pauls said if you use single records you get better performance\n",
    "\n",
    "            if map_name == 'clip_latent':\n",
    "                inputs = clip_latent.to(device).float().squeeze(1)\n",
    "            elif map_name == 'clip_last_hidden':\n",
    "                inputs = clip_last_hidden.to(device).float().squeeze(1)\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "            elif map_name == 'imagebind_latent':\n",
    "                inputs = imagebind_latent.to(device).float().squeeze(1)\n",
    "            elif map_name == 'imagebind_last_hidden':\n",
    "                inputs = imagebind_hidden.to(device).float().squeeze(1)\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "            else:\n",
    "                raise Exception('Invalid map_name')\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.mse_loss(outputs, labels) + model.l2_regularization() # TODO: Try contrastive loss or any other loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            \n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, epoch_loss / num_batches))\n",
    "\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, (voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden) in enumerate(val_dl):\n",
    "                \n",
    "                # Move tensors to the configured device\n",
    "                labels = voxels.to(device).float().mean(dim=1).squeeze(1) # TODO: I'm just taking the mean, Pauls said if you use single records you get better performance\n",
    "\n",
    "                if map_name == 'clip_latent':\n",
    "                    inputs = clip_latent.to(device).float().squeeze(1)\n",
    "                elif map_name == 'clip_last_hidden':\n",
    "                    inputs = clip_last_hidden.to(device).float().squeeze(1)\n",
    "                    inputs = inputs.view(inputs.shape[0], -1)\n",
    "                elif map_name == 'imagebind_latent':\n",
    "                    inputs = imagebind_latent.to(device).float().squeeze(1)\n",
    "                elif map_name == 'imagebind_last_hidden':\n",
    "                    inputs = imagebind_hidden.to(device).float().squeeze(1)\n",
    "                    inputs = inputs.view(inputs.shape[0], -1)\n",
    "                else:\n",
    "                    raise Exception('Invalid map_name')\n",
    "                \n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = torch.nn.functional.mse_loss(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "\n",
    "                if test_loss < best_loss:\n",
    "                    best_loss = test_loss\n",
    "                    if save:\n",
    "                        torch.save(model.state_dict(), 'mlpmodels/best_model_{}.ckpt'.format(map_name))\n",
    "                    preds = outputs.cpu().numpy()\n",
    "                    actuals = labels.cpu().numpy()\n",
    "\n",
    "            print('Epoch [{}/{}], Test Loss: {:.4f}'.format(epoch+1, num_epochs, test_loss / 1))\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    return preds, actuals, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.4620\n",
      "Epoch [1/30], Test Loss: 0.4479\n",
      "Epoch [2/30], Loss: 0.3430\n",
      "Epoch [2/30], Test Loss: 0.4568\n",
      "Epoch [3/30], Loss: 0.2724\n",
      "Epoch [3/30], Test Loss: 0.4498\n",
      "Epoch [4/30], Loss: 0.2199\n",
      "Epoch [4/30], Test Loss: 0.4400\n",
      "Epoch [5/30], Loss: 0.1921\n",
      "Epoch [5/30], Test Loss: 0.4387\n",
      "Epoch [6/30], Loss: 0.1789\n",
      "Epoch [6/30], Test Loss: 0.4444\n",
      "Epoch [7/30], Loss: 0.1767\n",
      "Epoch [7/30], Test Loss: 0.4657\n",
      "Epoch [8/30], Loss: 0.1851\n",
      "Epoch [8/30], Test Loss: 0.4649\n",
      "Epoch [9/30], Loss: 0.2045\n",
      "Epoch [9/30], Test Loss: 0.5216\n",
      "Epoch [10/30], Loss: 0.2182\n",
      "Epoch [10/30], Test Loss: 0.4690\n",
      "Epoch [11/30], Loss: 0.1970\n",
      "Epoch [11/30], Test Loss: 0.4977\n",
      "Epoch [12/30], Loss: 0.1905\n",
      "Epoch [12/30], Test Loss: 0.4871\n",
      "Epoch [13/30], Loss: 0.2066\n",
      "Epoch [13/30], Test Loss: 0.6574\n",
      "Epoch [14/30], Loss: 0.2571\n",
      "Epoch [14/30], Test Loss: 0.9533\n",
      "Epoch [15/30], Loss: 0.3754\n",
      "Epoch [15/30], Test Loss: 1.0909\n",
      "Epoch [16/30], Loss: 0.6796\n",
      "Epoch [16/30], Test Loss: 0.9593\n",
      "Epoch [17/30], Loss: 1.2183\n",
      "Epoch [17/30], Test Loss: 2.2353\n",
      "Epoch [18/30], Loss: 2.9112\n",
      "Epoch [18/30], Test Loss: 9.0486\n",
      "Epoch [19/30], Loss: 11.7694\n",
      "Epoch [19/30], Test Loss: 17.8392\n",
      "Epoch [20/30], Loss: 34.2680\n",
      "Epoch [20/30], Test Loss: 0.7177\n",
      "Epoch [21/30], Loss: 0.5005\n",
      "Epoch [21/30], Test Loss: 0.4771\n",
      "Epoch [22/30], Loss: 0.2961\n",
      "Epoch [22/30], Test Loss: 0.4522\n",
      "Epoch [23/30], Loss: 0.2235\n",
      "Epoch [23/30], Test Loss: 0.4410\n",
      "Epoch [24/30], Loss: 0.1830\n",
      "Epoch [24/30], Test Loss: 0.4346\n",
      "Epoch [25/30], Loss: 0.1568\n",
      "Epoch [25/30], Test Loss: 0.4305\n",
      "Epoch [26/30], Loss: 0.1389\n",
      "Epoch [26/30], Test Loss: 0.4272\n",
      "Epoch [27/30], Loss: 0.1262\n",
      "Epoch [27/30], Test Loss: 0.4257\n",
      "Epoch [28/30], Loss: 0.1304\n",
      "Epoch [28/30], Test Loss: 0.4309\n",
      "Epoch [29/30], Loss: 0.1759\n",
      "Epoch [29/30], Test Loss: 0.4448\n",
      "Epoch [30/30], Loss: 0.1586\n",
      "Epoch [30/30], Test Loss: 0.4779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20544/20544 [00:01<00:00, 17612.34it/s]\n",
      "100%|██████████| 19004/19004 [00:01<00:00, 18574.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  \n",
      "Right: 0.38833248187880753 \n",
      "Left: 0.3788003623493953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model and get the predictions for final clip\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\") # Change for the gpu you're using\n",
    "\n",
    "model = MLP(clip_last_hidden_shape[-1] * clip_last_hidden_shape[-2] , voxels_shape[-1], alpha = 1e-8)\n",
    "preds, actuals, model = train_the_model_and_get_predictions('clip_last_hidden', model, save = False)\n",
    "\n",
    "y_val_right_pred = preds[:, 20544:]\n",
    "y_val_left_pred = preds[:, :20544]\n",
    "\n",
    "rh_fmri_val = actuals[:, 20544:]\n",
    "lh_fmri_val = actuals[:, :20544]\n",
    "\n",
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(y_val_left_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_left_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(y_val_left_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(y_val_right_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_right_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(y_val_right_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "    \n",
    "print(\"Score: \", \"\\nRight:\", rh_correlation.mean(),\"\\nLeft:\", lh_correlation.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/38], Loss: 0.4616\n",
      "(300, 39548)\n",
      "Epoch [1/38], Test Loss: 0.4705\n",
      "Epoch [2/38], Loss: 0.4328\n",
      "(300, 39548)\n",
      "Epoch [2/38], Test Loss: 0.4508\n",
      "Epoch [3/38], Loss: 0.4177\n",
      "(300, 39548)\n",
      "Epoch [3/38], Test Loss: 0.4396\n",
      "Epoch [4/38], Loss: 0.4085\n",
      "(300, 39548)\n",
      "Epoch [4/38], Test Loss: 0.4327\n",
      "Epoch [5/38], Loss: 0.4017\n",
      "(300, 39548)\n",
      "Epoch [5/38], Test Loss: 0.4282\n",
      "Epoch [6/38], Loss: 0.3971\n",
      "(300, 39548)\n",
      "Epoch [6/38], Test Loss: 0.4247\n",
      "Epoch [7/38], Loss: 0.3942\n",
      "(300, 39548)\n",
      "Epoch [7/38], Test Loss: 0.4222\n",
      "Epoch [8/38], Loss: 0.3912\n",
      "(300, 39548)\n",
      "Epoch [8/38], Test Loss: 0.4207\n",
      "Epoch [9/38], Loss: 0.3896\n",
      "(300, 39548)\n",
      "Epoch [9/38], Test Loss: 0.4189\n",
      "Epoch [10/38], Loss: 0.3885\n",
      "(300, 39548)\n",
      "Epoch [10/38], Test Loss: 0.4179\n",
      "Epoch [11/38], Loss: 0.3868\n",
      "(300, 39548)\n",
      "Epoch [11/38], Test Loss: 0.4169\n",
      "Epoch [12/38], Loss: 0.3857\n",
      "(300, 39548)\n",
      "Epoch [12/38], Test Loss: 0.4159\n",
      "Epoch [13/38], Loss: 0.3851\n",
      "(300, 39548)\n",
      "Epoch [13/38], Test Loss: 0.4155\n",
      "Epoch [14/38], Loss: 0.3840\n",
      "(300, 39548)\n",
      "Epoch [14/38], Test Loss: 0.4149\n",
      "Epoch [15/38], Loss: 0.3829\n",
      "(300, 39548)\n",
      "Epoch [15/38], Test Loss: 0.4147\n",
      "Epoch [16/38], Loss: 0.3826\n",
      "(300, 39548)\n",
      "Epoch [16/38], Test Loss: 0.4142\n",
      "Epoch [17/38], Loss: 0.3811\n",
      "(300, 39548)\n",
      "Epoch [17/38], Test Loss: 0.4140\n",
      "Epoch [18/38], Loss: 0.3813\n",
      "(300, 39548)\n",
      "Epoch [18/38], Test Loss: 0.4137\n",
      "Epoch [19/38], Loss: 0.3806\n",
      "(300, 39548)\n",
      "Epoch [19/38], Test Loss: 0.4136\n",
      "Epoch [20/38], Loss: 0.3807\n",
      "(300, 39548)\n",
      "Epoch [20/38], Test Loss: 0.4136\n",
      "Epoch [21/38], Loss: 0.3794\n",
      "(300, 39548)\n",
      "Epoch [21/38], Test Loss: 0.4133\n",
      "Epoch [22/38], Loss: 0.3797\n",
      "(300, 39548)\n",
      "Epoch [22/38], Test Loss: 0.4133\n",
      "Epoch [23/38], Loss: 0.3793\n",
      "(300, 39548)\n",
      "Epoch [23/38], Test Loss: 0.4132\n",
      "Epoch [24/38], Loss: 0.3781\n",
      "(300, 39548)\n",
      "Epoch [24/38], Test Loss: 0.4132\n",
      "Epoch [25/38], Loss: 0.3789\n",
      "(300, 39548)\n",
      "Epoch [25/38], Test Loss: 0.4130\n",
      "Epoch [26/38], Loss: 0.3786\n",
      "Epoch [26/38], Test Loss: 0.4131\n",
      "Epoch [27/38], Loss: 0.3783\n",
      "(300, 39548)\n",
      "Epoch [27/38], Test Loss: 0.4128\n",
      "Epoch [28/38], Loss: 0.3781\n",
      "Epoch [28/38], Test Loss: 0.4129\n",
      "Epoch [29/38], Loss: 0.3781\n",
      "(300, 39548)\n",
      "Epoch [29/38], Test Loss: 0.4128\n",
      "Epoch [30/38], Loss: 0.3784\n",
      "Epoch [30/38], Test Loss: 0.4130\n",
      "Epoch [31/38], Loss: 0.3774\n",
      "Epoch [31/38], Test Loss: 0.4130\n",
      "Epoch [32/38], Loss: 0.3776\n",
      "(300, 39548)\n",
      "Epoch [32/38], Test Loss: 0.4128\n",
      "Epoch [33/38], Loss: 0.3771\n",
      "Epoch [33/38], Test Loss: 0.4128\n",
      "Epoch [34/38], Loss: 0.3773\n",
      "Epoch [34/38], Test Loss: 0.4129\n",
      "Epoch [35/38], Loss: 0.3773\n",
      "Epoch [35/38], Test Loss: 0.4132\n",
      "Epoch [36/38], Loss: 0.3767\n",
      "Epoch [36/38], Test Loss: 0.4129\n",
      "Epoch [37/38], Loss: 0.3770\n",
      "Epoch [37/38], Test Loss: 0.4128\n",
      "Epoch [38/38], Loss: 0.3770\n",
      "Epoch [38/38], Test Loss: 0.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20544/20544 [00:02<00:00, 9140.29it/s]\n",
      "100%|██████████| 19004/19004 [00:01<00:00, 9769.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.41466480265972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds, actuals, model = train_the_model_and_get_predictions('imagebind_latent')\n",
    "y_val_right_pred = preds[:, 20544:]\n",
    "y_val_left_pred = preds[:, :20544]\n",
    "\n",
    "rh_fmri_val = actuals[:, 20544:]\n",
    "lh_fmri_val = actuals[:, :20544]\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr as corr\n",
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(y_val_left_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_left_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(y_val_left_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(y_val_right_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_right_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(y_val_right_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "    \n",
    "print(\" \", rh_correlation.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/38], Loss: 234.9764\n",
      "(300, 39548)\n",
      "Epoch [1/38], Test Loss: 21.0682\n",
      "Epoch [2/38], Loss: 29.2692\n",
      "(300, 39548)\n",
      "Epoch [2/38], Test Loss: 0.9949\n",
      "Epoch [3/38], Loss: 0.7374\n",
      "(300, 39548)\n",
      "Epoch [3/38], Test Loss: 0.7212\n",
      "Epoch [4/38], Loss: 0.5355\n",
      "(300, 39548)\n",
      "Epoch [4/38], Test Loss: 0.6501\n",
      "Epoch [5/38], Loss: 0.4738\n",
      "(300, 39548)\n",
      "Epoch [5/38], Test Loss: 0.6129\n",
      "Epoch [6/38], Loss: 0.4417\n",
      "(300, 39548)\n",
      "Epoch [6/38], Test Loss: 0.5825\n",
      "Epoch [7/38], Loss: 0.4173\n",
      "(300, 39548)\n",
      "Epoch [7/38], Test Loss: 0.5575\n",
      "Epoch [8/38], Loss: 0.3944\n",
      "(300, 39548)\n",
      "Epoch [8/38], Test Loss: 0.5360\n",
      "Epoch [9/38], Loss: 0.3761\n",
      "(300, 39548)\n",
      "Epoch [9/38], Test Loss: 0.5171\n",
      "Epoch [10/38], Loss: 0.3588\n",
      "(300, 39548)\n",
      "Epoch [10/38], Test Loss: 0.5022\n",
      "Epoch [11/38], Loss: 0.3443\n",
      "(300, 39548)\n",
      "Epoch [11/38], Test Loss: 0.4858\n",
      "Epoch [12/38], Loss: 0.3299\n",
      "(300, 39548)\n",
      "Epoch [12/38], Test Loss: 0.4736\n",
      "Epoch [13/38], Loss: 0.3181\n",
      "(300, 39548)\n",
      "Epoch [13/38], Test Loss: 0.4621\n",
      "Epoch [14/38], Loss: 0.3074\n",
      "(300, 39548)\n",
      "Epoch [14/38], Test Loss: 0.4516\n",
      "Epoch [15/38], Loss: 0.2970\n",
      "(300, 39548)\n",
      "Epoch [15/38], Test Loss: 0.4411\n",
      "Epoch [16/38], Loss: 0.2877\n",
      "(300, 39548)\n",
      "Epoch [16/38], Test Loss: 0.4320\n",
      "Epoch [17/38], Loss: 0.2795\n",
      "(300, 39548)\n",
      "Epoch [17/38], Test Loss: 0.4236\n",
      "Epoch [18/38], Loss: 0.2715\n",
      "(300, 39548)\n",
      "Epoch [18/38], Test Loss: 0.4166\n",
      "Epoch [19/38], Loss: 0.2656\n",
      "(300, 39548)\n",
      "Epoch [19/38], Test Loss: 0.4106\n",
      "Epoch [20/38], Loss: 0.2606\n",
      "(300, 39548)\n",
      "Epoch [20/38], Test Loss: 0.4048\n",
      "Epoch [21/38], Loss: 0.2572\n",
      "(300, 39548)\n",
      "Epoch [21/38], Test Loss: 0.4014\n",
      "Epoch [22/38], Loss: 0.2568\n",
      "Epoch [22/38], Test Loss: 0.4075\n",
      "Epoch [23/38], Loss: 0.2763\n",
      "Epoch [23/38], Test Loss: 0.4839\n",
      "Epoch [24/38], Loss: 0.3652\n",
      "Epoch [24/38], Test Loss: 0.4484\n",
      "Epoch [25/38], Loss: 0.2819\n",
      "Epoch [25/38], Test Loss: 0.4602\n",
      "Epoch [26/38], Loss: 0.3607\n",
      "Epoch [26/38], Test Loss: 0.5891\n",
      "Epoch [27/38], Loss: 0.3866\n",
      "(300, 39548)\n",
      "Epoch [27/38], Test Loss: 0.3921\n",
      "Epoch [28/38], Loss: 0.2701\n",
      "(300, 39548)\n",
      "Epoch [28/38], Test Loss: 0.3795\n",
      "Epoch [29/38], Loss: 0.2547\n",
      "(300, 39548)\n",
      "Epoch [29/38], Test Loss: 0.3765\n",
      "Epoch [30/38], Loss: 0.2375\n",
      "(300, 39548)\n",
      "Epoch [30/38], Test Loss: 0.3736\n",
      "Epoch [31/38], Loss: 0.2272\n",
      "(300, 39548)\n",
      "Epoch [31/38], Test Loss: 0.3659\n",
      "Epoch [32/38], Loss: 0.2192\n",
      "Epoch [32/38], Test Loss: 0.3686\n",
      "Epoch [33/38], Loss: 0.2139\n",
      "(300, 39548)\n",
      "Epoch [33/38], Test Loss: 0.3600\n",
      "Epoch [34/38], Loss: 0.2056\n",
      "(300, 39548)\n",
      "Epoch [34/38], Test Loss: 0.3577\n",
      "Epoch [35/38], Loss: 0.2016\n",
      "(300, 39548)\n",
      "Epoch [35/38], Test Loss: 0.3575\n",
      "Epoch [36/38], Loss: 0.2011\n",
      "Epoch [36/38], Test Loss: 0.3604\n",
      "Epoch [37/38], Loss: 0.2061\n",
      "Epoch [37/38], Test Loss: 0.3776\n",
      "Epoch [38/38], Loss: 0.2284\n",
      "Epoch [38/38], Test Loss: 0.3806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20544/20544 [00:01<00:00, 19968.88it/s]\n",
      "100%|██████████| 19004/19004 [00:00<00:00, 20571.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.530479848655317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model and get the predictions\n",
    "preds, actuals, model = train_the_model_and_get_predictions('clip_last_hidden')\n",
    "\n",
    "y_val_right_pred = preds[:, 20544:]\n",
    "y_val_left_pred = preds[:, :20544]\n",
    "\n",
    "rh_fmri_val = actuals[:, 20544:]\n",
    "lh_fmri_val = actuals[:, :20544]\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr as corr\n",
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(y_val_left_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_left_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(y_val_left_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(y_val_right_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_right_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(y_val_right_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "    \n",
    "print(\" \", rh_correlation.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
