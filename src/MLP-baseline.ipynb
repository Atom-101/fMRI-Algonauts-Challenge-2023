{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "- Improve training features (accelerator, lr scheduler, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pulling NSD webdataset data...')\n",
    "# Note: using \"voxel\" naming even though we use vertices here... makes it easier porting over MindEye lingo\n",
    "\n",
    "train_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/subj01_{3..98}.tar\"\n",
    "val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/subj01_{0..2}.tar\"\n",
    "meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/algonauts_data/wds/metadata_subj01.json\"\n",
    "\n",
    "metadata = json.load(open(meta_url))\n",
    "num_train = metadata['total'] - 300\n",
    "num_val = 300\n",
    "batch_size = 32\n",
    "num_devices = 1\n",
    "seed = 42\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "import math\n",
    "import random\n",
    "import webdataset as wds\n",
    "def my_split_by_node(urls):\n",
    "    return urls\n",
    "\n",
    "num_workers = 10\n",
    "\n",
    "global_batch_size = batch_size * num_devices\n",
    "num_batches = math.floor(num_train / global_batch_size)\n",
    "num_worker_batches = math.floor(num_batches / num_workers)\n",
    "\n",
    "train_data = wds.WebDataset(train_url, resampled=False)\\\n",
    "    .shuffle(500, initial=500, rng=random.Random(seed))\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=\"vert.npy\", clip_latent=\"clip_emb_final.npy\", clip_last_hidden=\"clip_emb_hidden.npy\", imagebind_latent='imagebind_final.npy', imagebinde_hidden='imagebind_hidden.npy')\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"clip_latent\", \"clip_last_hidden\", \"imagebind_latent\", \"imagebinde_hidden\")\\\n",
    "    .batched(batch_size, partial=False)\\\n",
    "    .with_epoch(num_worker_batches)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_data, num_workers=num_workers,\n",
    "                        batch_size=None, shuffle=False, persistent_workers=True)\n",
    "\n",
    "global_batch_size = batch_size\n",
    "num_workers_val = 1\n",
    "\n",
    "num_batches_val = math.ceil(num_val / global_batch_size)\n",
    "num_worker_batches_val = math.ceil(num_batches_val / num_workers)\n",
    "print(\"validation: num_worker_batches\", num_worker_batches_val)\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False, nodesplitter=my_split_by_node)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=\"vert.npy\", clip_latent=\"clip_emb_final.npy\", clip_last_hidden=\"clip_emb_hidden.npy\", imagebind_latent='imagebind_final.npy', imagebinde_hidden='imagebind_hidden.npy')\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"clip_latent\", \"clip_last_hidden\", \"imagebind_latent\", \"imagebinde_hidden\")\\\n",
    "    .batched(300, partial=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_data, num_workers=num_workers_val,\n",
    "                    batch_size=None, shuffle=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxels_shape, images_shape, clip_latent_shape, clip_last_hidden_shape, imagebind_latent_shape, imagebind_hidden_shape = None, None, None, None, None, None\n",
    "for voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden in tqdm(train_dl):\n",
    "    voxels_shape = voxels.shape\n",
    "    images_shape = images.shape\n",
    "    clip_latent_shape = clip_latent.shape\n",
    "    clip_last_hidden_shape = clip_last_hidden.shape\n",
    "    imagebind_latent_shape = imagebind_latent.shape\n",
    "    imagebind_hidden_shape = imagebind_hidden.shape\n",
    "    break\n",
    "\n",
    "print('Val dataloader shapes:')\n",
    "print('voxels', voxels_shape)\n",
    "print('images', images_shape)\n",
    "print('clip_latent', clip_latent_shape)\n",
    "print('clip_last_hidden', clip_last_hidden_shape)\n",
    "print('imagebind_latent', imagebind_latent_shape)\n",
    "print('imagebind_hidden', imagebind_hidden_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxels_shape, images_shape, clip_latent_shape, clip_last_hidden_shape, imagebind_latent_shape, imagebind_hidden_shape = None, None, None, None, None, None\n",
    "for voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden in val_dl:\n",
    "    voxels_shape = voxels.shape\n",
    "    images_shape = images.shape\n",
    "    clip_latent_shape = clip_latent.shape\n",
    "    clip_last_hidden_shape = clip_last_hidden.shape\n",
    "    imagebind_latent_shape = imagebind_latent.shape\n",
    "    imagebind_hidden_shape = imagebind_hidden.shape\n",
    "    break\n",
    "\n",
    "print('Val dataloader shapes:')\n",
    "print('voxels', voxels_shape)\n",
    "print('images', images_shape)\n",
    "print('clip_latent', clip_latent_shape)\n",
    "print('clip_last_hidden', clip_last_hidden_shape)\n",
    "print('imagebind_latent', imagebind_latent_shape)\n",
    "print('imagebind_hidden', imagebind_hidden_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "alpha = 1e-8\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=5000):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_in = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear_hid = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_hid(self.relu(self.linear_in(x)))\n",
    "\n",
    "\n",
    "\n",
    "def train_the_model_and_get_predictions(map_name, model, save = True):\n",
    "    # outputs name can be 'clip_latent', 'clip_last_hidden', 'imagebind_latent', 'imagebind_hidden'\n",
    "    map_shape = None\n",
    "    if map_name == 'clip_latent':\n",
    "        map_shape = clip_latent_shape\n",
    "    elif map_name == 'clip_last_hidden':\n",
    "        map_shape = (clip_last_hidden_shape[0], clip_last_hidden_shape[1], clip_last_hidden_shape[2] * clip_last_hidden_shape[3])\n",
    "    elif map_name == 'imagebind_latent':\n",
    "        map_shape = imagebind_latent_shape\n",
    "    elif map_name == 'imagebind_last_hidden':\n",
    "        map_shape = (clip_last_hidden_shape[0], imagebind_hidden_shape[1], imagebind_hidden_shape[2] * imagebind_hidden_shape[3])\n",
    "    else:\n",
    "        raise Exception('Invalid map_name')\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    best_loss = 1000000\n",
    "\n",
    "    # Train the right hemisphere model\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden) in enumerate(train_dl):\n",
    "\n",
    "            # Move tensors to the configured device\n",
    "            labels = voxels.to(device).float().mean(dim=1)  # TODO: I'm just taking the mean, Pauls said if you use single records you get better performance\n",
    "\n",
    "            if map_name == 'clip_latent':\n",
    "                inputs = clip_latent.to(device).float().squeeze(1)\n",
    "            elif map_name == 'clip_last_hidden':\n",
    "                inputs = clip_last_hidden.to(device).float().squeeze(1)\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "            elif map_name == 'imagebind_latent':\n",
    "                inputs = imagebind_latent.to(device).float().squeeze(1)\n",
    "            elif map_name == 'imagebind_last_hidden':\n",
    "                inputs = imagebind_hidden.to(device).float().squeeze(1)\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "            else:\n",
    "                raise Exception('Invalid map_name')\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.mse_loss(outputs, labels) # TODO: Try contrastive loss or any other loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            \n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, epoch_loss / num_batches))\n",
    "\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, (voxels, images, clip_latent, clip_last_hidden, imagebind_latent, imagebind_hidden) in enumerate(val_dl):\n",
    "                \n",
    "                # Move tensors to the configured device\n",
    "                labels = voxels.to(device).float().mean(dim=1).squeeze(1) # TODO: I'm just taking the mean, Pauls said if you use single records you get better performance\n",
    "\n",
    "                if map_name == 'clip_latent':\n",
    "                    inputs = clip_latent.to(device).float().squeeze(1)\n",
    "                elif map_name == 'clip_last_hidden':\n",
    "                    inputs = clip_last_hidden.to(device).float().squeeze(1)\n",
    "                    inputs = inputs.view(inputs.shape[0], -1)\n",
    "                elif map_name == 'imagebind_latent':\n",
    "                    inputs = imagebind_latent.to(device).float().squeeze(1)\n",
    "                elif map_name == 'imagebind_last_hidden':\n",
    "                    inputs = imagebind_hidden.to(device).float().squeeze(1)\n",
    "                    inputs = inputs.view(inputs.shape[0], -1)\n",
    "                else:\n",
    "                    raise Exception('Invalid map_name')\n",
    "                \n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = torch.nn.functional.mse_loss(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "\n",
    "                if test_loss < best_loss:\n",
    "                    best_loss = test_loss\n",
    "                    if save:\n",
    "                        torch.save(model.state_dict(), 'mlpmodels/best_model_{}.ckpt'.format(map_name))\n",
    "                    preds = outputs.cpu().numpy()\n",
    "                    actuals = labels.cpu().numpy()\n",
    "\n",
    "            print('Epoch [{}/{}], Test Loss: {:.4f}'.format(epoch+1, num_epochs, test_loss / 1))\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    return preds, actuals, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and get the predictions for final clip\n",
    "\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\") # Change for the gpu you're using\n",
    "\n",
    "model = MLP(clip_last_hidden_shape[-1] * clip_last_hidden_shape[-2] , voxels_shape[-1])\n",
    "preds, actuals, model = train_the_model_and_get_predictions('clip_last_hidden', model, save = False)\n",
    "\n",
    "y_val_right_pred = preds[:, 20544:]\n",
    "y_val_left_pred = preds[:, :20544]\n",
    "\n",
    "rh_fmri_val = actuals[:, 20544:]\n",
    "lh_fmri_val = actuals[:, :20544]\n",
    "\n",
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(y_val_left_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_left_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(y_val_left_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(y_val_right_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_right_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(y_val_right_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "    \n",
    "print(\"Score: \", \"\\nRight:\", rh_correlation.mean(),\"\\nLeft:\", lh_correlation.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and get the predictions for final clip\n",
    "\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\") # Change for the gpu you're using\n",
    "\n",
    "model = MLP(clip_last_hidden_shape[-1] * clip_last_hidden_shape[-2] , voxels_shape[-1])\n",
    "preds, actuals, model = train_the_model_and_get_predictions('imagebind_last_hidden', model, save = False)\n",
    "\n",
    "y_val_right_pred = preds[:, 20544:]\n",
    "y_val_left_pred = preds[:, :20544]\n",
    "\n",
    "rh_fmri_val = actuals[:, 20544:]\n",
    "lh_fmri_val = actuals[:, :20544]\n",
    "\n",
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(y_val_left_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_left_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(y_val_left_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(y_val_right_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(y_val_right_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(y_val_right_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "    \n",
    "print(\"Score: \", \"\\nRight:\", rh_correlation.mean(),\"\\nLeft:\", lh_correlation.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
